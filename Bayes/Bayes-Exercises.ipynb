{"cells":[{"cell_type":"markdown","source":["# David Böhler, Nico Knünz, Julian Mathis"],"metadata":{"id":"XuoD6d1eYMfx"}},{"cell_type":"markdown","metadata":{"id":"kGxRCUJfiIyx"},"source":["# Bayes Exercises\n","\n","\n","**Hints**:\n","\n","In many ML libraries we use the terminology ```X``` for features and ```Y``` for labels/annotations\n","or target values. X and Y are often numpy arrays. Such that Y is a column vector and X is a multidimensional array with:\n","* the first dimension represents each sample\n","* the remaining index dimensions represent the feature-values (these can be multi-dimensional in case we use images)\n","\n","The number of samples and the number of labels (i.e.: the number of entries in the first dimension of X and Y) must be the same.\n","Acessing the 13-th feature vector and corresponding class in our training data:\n","```python\n","x13 = X[12, :] # index starts with 0\n","y13 = Y[12]\n","```\n","\n","Using logical indexing could come in to be really helpful here:\n","```python\n","    X[Y == 0,:]  # Fetch all features (from X) according to class label 0\n","    X[Y == 1,:]  # Fetch all features (from X) according to class label 1\n","```\n","\n","Also:\n","* ```np.unique()```\n","* ```np.sum()``` (have a look at the **axis** parameter)\n","* ``` X.reshape()```\n","\n","could be helpful.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7-O5lyUAiIy0","executionInfo":{"status":"ok","timestamp":1732841127228,"user_tz":-60,"elapsed":344,"user":{"displayName":"Julian Mathis","userId":"08151527641515458881"}}},"outputs":[],"source":["# This cell generates your test values (they are the same as in the decision-tree exercise).\n","#\n","\n","import numpy as np\n","# Test features from previous exercise.\n","#\n","X_train = np.array([\n","                    [2, 1, 3],       # Class 1: Red, Green, Blue\n","                    [10, 30, 20],    # Class 2: Red, Green, Blue\n","                    [1, 3, 2],       # Class 2: Red, Green, Blue\n","                    [40, 20, 60]     # Class 1: Red, Green, Blue\n","                 ], dtype=\"float32\")\n","\n","# Labels for each feature in X_train\n","#\n","Y_train = np.array([0, 1, 1, 0])"]},{"cell_type":"markdown","metadata":{"id":"n6zFvkhiiIy1"},"source":["### Exercise 1: Implementation of calculating the prior values.\n","\n","* In this exercise your job is to implement the calculation prior values for each class.\n","\n","\n","\n","**Use the following stubs for your implementation:**\n","\n","\n","```python\n","def compute_priors(Y):\n","    \"\"\" Compute the priors per class in Y.\n","\n","    Parameters\n","    ----------\n","    Y: np.array\n","      A one dimensional numpy array containing class labels.\n","\n","    Returns\n","    ----------\n","    A dictionary containing the priors per class.\n","\n","    Expected Output for X_train, Y_train\n","    ----------\n","    {0: 0.5, 1: 0.5}\n","    \"\"\"\n","```\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"49MUYDvViIy2","executionInfo":{"status":"ok","timestamp":1732841127574,"user_tz":-60,"elapsed":3,"user":{"displayName":"Julian Mathis","userId":"08151527641515458881"}}},"outputs":[],"source":["def compute_priors(Y):\n","  labels = np.unique(Y)\n","  #print(labels)\n","  dict = {}\n","  for y in labels:\n","    dict[y] = np.size(Y_train[Y == y])  / np.size(Y_train)\n","\n","  return dict"]},{"cell_type":"code","source":["priors = compute_priors(Y_train)\n","print(priors)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eeguT1uaOu4C","executionInfo":{"status":"ok","timestamp":1732841141951,"user_tz":-60,"elapsed":345,"user":{"displayName":"Julian Mathis","userId":"08151527641515458881"}},"outputId":"adb642d5-8c31-4f6e-ce18-4800fe0d5ebb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 0.5, 1: 0.5}\n"]}]},{"cell_type":"markdown","metadata":{"id":"thOoHdLtiIy2"},"source":["### Exercise 2: Implementation of calculating the pik values.\n","\n","**In this exercise your job is to implement the calculation of the probability values (pik) for each feature attribute under each class.**\n","\n","\n","**Warning:**\n","\n","Be careful when computing the probabilities. We do not want larger images (more pixels and therefore more entries in a non-normalized histogram) to have a larger impact.\n","Have a look at the features, you might have to normalize the individual feature vectors.\n","\n","**Use the following stubs for your implementation:**\n","\n","```python\n","def compute_pik(X, Y):\n","   \"\"\" Compute the probabilities per feature-dimension (pik).\n","\n","    Parameters\n","    ----------\n","    X: np.array\n","      A two dimensional numpy array containing feature vectors (in rows) for each sample\n","      in the training data.\n","    Y: np.array\n","      A one dimensional numpy array containing class labels.\n","\n","    Returns\n","    ----------\n","    A dictionary containing the piks per class where keys are the class labels and values are the pik values.\n","\n","    Expected Output for X_train, Y_train\n","    ----------\n","    {0: array([0.33333333, 0.16666667, 0.5       ]),\n","     1: array([0.16666667, 0.5       , 0.33333333])}\n","    \"\"\"\n","```\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ThxIZkf7iIy2","executionInfo":{"status":"ok","timestamp":1732841171007,"user_tz":-60,"elapsed":215,"user":{"displayName":"Julian Mathis","userId":"08151527641515458881"}}},"outputs":[],"source":["def compute_pik(X, Y):\n","  labels = np.unique(Y)\n","  #print(labels)\n","  dict = {}\n","  for y in labels:\n","    #print(y)\n","    array = np.zeros(X.shape[1])\n","    count = 0\n","    for x in X[Y==y]:\n","      #normalize and add to zero array\n","      #print( x / np.sum(x))\n","      array = np.add(array, x / np.sum(x))\n","      count = count + 1\n","    #print(array)\n","    #divide for mean\n","    dict[y] = array / count\n","  return dict\n"]},{"cell_type":"code","source":["print(compute_pik(X_train, Y_train))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5n6n8kCPUKG_","executionInfo":{"status":"ok","timestamp":1732841173278,"user_tz":-60,"elapsed":210,"user":{"displayName":"Julian Mathis","userId":"08151527641515458881"}},"outputId":"8d44cf04-dfe3-4cfc-879e-2b6d59172cab"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: array([0.33333334, 0.16666667, 0.5       ]), 1: array([0.16666667, 0.5       , 0.33333334])}\n"]}]},{"cell_type":"markdown","metadata":{"id":"IkvK5IkBiIy2"},"source":["### Exercise 3: Implement the naive Bayes Classifier\n","\n","* Implement the Bayes classifier without using the log trick.\n","* Implement the Bayes classifier with using the log trick.\n","* Compare your results.\n","\n","Hint:\n","* ```np.power()``` to compute $p_{ik}^{x_i}$ in a single instruction could be helpful\n","* ```np.product()``` to compute the product of a list of values could be helpful\n","\n","**Use the following stubs for your implementation:**\n","```python\n","def classify_bayes(x, piks, priors):\n","  \"\"\" Compute the posterior probability for a feature vector and perform classification according to naive Bayes.\n","\n","  Parameters\n","  ----------\n","  x: np.array\n","    A one dimensional numpy array containing a single feature vector.\n","  piks: dict\n","    A dictionary of likelihoods as computed by compute_piks()\n","  priors: dict\n","    A dictionary of priors as computed by compute_priors()\n","\n","  Returns\n","  ----------\n","  Class label of the most probable class and a dictionary where the key is the class and the value is the posterior.\n","\n","  Expected Output for X_test[0,:]\n","  ----------\n","  (0, {0: 0.002057613168724279, 1: 6.430041152263372e-05})\n","  \"\"\"\n","  pass\n","```\n","\n","**and**\n","\n","```python\n","def classify_bayes_log(x, piks, priors):\n","  \"\"\" Compute the log-posterior probability for a feature vector and perform classification according to naive Bayes.\n","\n","  Parameters\n","  ----------\n","  x: np.array\n","    A one dimensional numpy array containing a single feature vector.\n","  piks: dict\n","    A dictionary of likelihoods as computed by compute_piks()\n","  priors: dict\n","    A dictionary of priors as computed by compute_priors()\n","\n","  Returns\n","  ----------\n","  Class label of the most probable class and a dictionary where the key is the class and the value is the posterior.\n","\n","  Expected Output for X_test[0,:]\n","  ----------\n","  (0, {0: -6.1862086239004945, 1: -9.65194452670022})\n","  \"\"\"\n","```\n","\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"PPq6NczPiIy3","executionInfo":{"status":"ok","timestamp":1732841201827,"user_tz":-60,"elapsed":207,"user":{"displayName":"Julian Mathis","userId":"08151527641515458881"}}},"outputs":[],"source":["# Use this X_test to verify your implementation.\n","#\n","X_test = np.array([\n","                    [5, 0, 0],    # x1\n","                    [2, 1, 3],    # x2\n","                    [2, 7, 4]     # x3\n","                 ])"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"-sEFcHpmiIy3","executionInfo":{"status":"ok","timestamp":1732841474836,"user_tz":-60,"elapsed":203,"user":{"displayName":"Julian Mathis","userId":"08151527641515458881"}}},"outputs":[],"source":["import math\n","\n","def classify_bayes(x, piks, priors):\n","  maximum = -1.\n","  dict = {}\n","  arrayL = []\n","  arrayB = []\n","  for entry in priors.keys():\n","    prior = priors[entry]\n","    pik = piks[entry]\n","    #every pik to the power of its count drawn\n","    exponented = np.power(pik,x)\n","    #multiply them together\n","    likely = np.prod(exponented)\n","    #\n","    post = prior * likely\n","\n","    dict[entry] = post\n","    arrayB = np.append(arrayB, post)\n","    arrayL = np.append(arrayL, entry)\n","  maxIndex = np.argmax(arrayB)\n","  choose = int(arrayL[maxIndex])\n","\n","  return (choose, dict)\n","\n","def classify_bayes_log(x, piks, priors):\n","  maximum = -1.\n","  dict = {}\n","  arrayL = []\n","  arrayB = []\n","  for entry in priors.keys():\n","    prior = priors[entry]\n","    pik = piks[entry]\n","    likelyTimesDrawn = np.log(pik) * x\n","    sum = np.sum(likelyTimesDrawn)\n","    post = math.log(prior) + sum\n","    dict[entry] = post\n","    arrayB = np.append(arrayB, post)\n","    arrayL = np.append(arrayL, entry)\n","  maxIndex = np.argmax(arrayB)\n","  choose = int(arrayL[maxIndex])\n","  return (choose, dict)"]},{"cell_type":"code","source":["priors = compute_priors(Y_train)\n","piks = compute_pik(X_train, Y_train)\n","\n","print(\"normal\")\n","print(classify_bayes(X_test[0], piks, priors))\n","print(classify_bayes(X_test[1], piks, priors))\n","print(classify_bayes(X_test[2], piks, priors))\n","print(\"\")\n","print(\"with log\")\n","print(classify_bayes_log(X_test[0], piks, priors))\n","print(classify_bayes_log(X_test[1], piks, priors))\n","print(classify_bayes_log(X_test[2], piks, priors))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3RCxt7djk83","executionInfo":{"status":"ok","timestamp":1732841476545,"user_tz":-60,"elapsed":197,"user":{"displayName":"Julian Mathis","userId":"08151527641515458881"}},"outputId":"4ff4c429-420e-4fa4-b9a0-ce228012e182"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["normal\n","(0, {0: 0.002057613475332553, 1: 6.430042110414228e-05})\n","(0, {0: 0.0011574075108876966, 1: 0.00025720168441656913})\n","(1, {0: 1.2403632092853037e-08, 1: 1.3395921462592534e-06})\n","\n","with log\n","(0, {0: -6.186208474888884, 1: -9.65194437768861})\n","(0, {0: -6.76157267939709, 1: -8.26565001656872})\n","(1, {0: -18.20527649651143, 1: -13.52314535879418})\n"]}]},{"cell_type":"markdown","source":["## Answer: Compare your results\n","\n","The normal calculation gets really small numbers, which can be hard to compare and can create errors with underflows. In comparison the log calculation has numbers in a reasnable range, whithout the chance for underflow or overflow.\n","\n","The log result is also a lot easier to read and to see which class should be assigned."],"metadata":{"id":"dNhwJ8MpwR0q"}}],"metadata":{"celltoolbar":"Edit Metadata","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"vscode":{"interpreter":{"hash":"71e3996440a4286f4b5430a3d4d1ae66fa68d894e5edac3334c7ebe5f98b7546"}},"colab":{"provenance":[{"file_id":"https://github.com/shegenbart/Jupyter-Exercises/blob/main/Bayes/Bayes-Exercises.ipynb","timestamp":1730192911915}]}},"nbformat":4,"nbformat_minor":0}